{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfb9b65-378e-4cb4-ae3f-2da5f244a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "# ðŸ”’ Reproducibility setup\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b90504-890f-48d8-8e03-8466607d654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42  # Choose any number you want, but be consistent\n",
    "# Set seeds\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# (Optional) Force single-threaded operations\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'  # Try to make TF ops deterministic\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36670e1-d04a-4b93-99d0-0efac4d460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If GPU is available \n",
    "# Step 1: Set GPU environment (should still be early, before TF uses GPU)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Step 2: GPU setup for Jupyter (safe inside notebook)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"âœ… GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"âš ï¸ RuntimeError during GPU setup:\", e)\n",
    "else:\n",
    "    print(\"âŒ No GPU found.\")\n",
    "\n",
    "# Step 3: Show detected GPUs\n",
    "print(\"Available GPU(s):\", tf.config.list_logical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7420776a-56d5-4b96-9d93-da79ece7782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In a new cell at the top of your notebook\n",
    "parameters = dict()\n",
    "\n",
    "# --- Model Architecture ---\n",
    "parameters['module'] = 'gru'\n",
    "parameters['hidden_dim'] = 64\n",
    "parameters['num_layers'] = 3\n",
    "parameters['z_dim'] = 64\n",
    "parameters['seq_len'] = 24      #This is the output_dim for the Embedder     ??????????????? always\n",
    "parameters['dim'] = 14    # This is the output_dim for the Recovery (your original feature count)   ???????????????\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "parameters['iterations'] = 4000\n",
    "parameters['batch_size'] = 128\n",
    "parameters['learning_rate'] = 0.001\n",
    "parameters['embedder_iterations'] = 4000\n",
    "parameters['supervisor_iterations'] = 5000\n",
    "\n",
    "# --- Loss Weights ---\n",
    "parameters['gamma'] = 10\n",
    "parameters['eta'] = 1\n",
    "parameters['lambda'] = 10\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764cf00f-32ba-4281-a014-086a11e10f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(data, batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data.astype('float32'))\n",
    "    ds = ds.shuffle(buffer_size=min(len(data), 10000), reshuffle_each_iteration=True)\n",
    "    ds = ds.repeat()                               # infinite\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    ds = ds.prefetch(1)                            # small, predictable prefetch\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910b6926-eaa7-4356-beb4-19c3b4c31420",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embedder(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, num_layers, output_dim, module_name):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        # Determine which RNN cell to use based on the module_name\n",
    "        if module_name == 'gru':\n",
    "            rnn_cell = layers.GRU\n",
    "        elif module_name == 'lstm':\n",
    "            rnn_cell = layers.LSTM\n",
    "        elif module_name == 'lstmLN':\n",
    "            rnn_cell = layers.lstmLN\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN module: {module_name}\")\n",
    "            \n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(hidden_units, return_sequences=True) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = layers.Dense(output_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        # Pass the input through the stack of RNN layers\n",
    "        for layer in self.rnn_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea40779a-0759-4a20-9c08-d436a4cdf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recovery(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, num_layers, output_dim, module_name):\n",
    "        super(Recovery, self).__init__()\n",
    "\n",
    "        # Determine which RNN cell to use based on the module_name\n",
    "        if module_name == 'gru':\n",
    "            rnn_cell = layers.GRU\n",
    "        elif module_name == 'lstm':\n",
    "            rnn_cell = layers.LSTM\n",
    "        elif module_name == 'lstmLN':\n",
    "            rnn_cell = layers.lstmLN\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN module: {module_name}\")\n",
    "\n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(hidden_units, return_sequences=True) for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = layers.Dense(output_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, h, training=True):\n",
    "        for layer in self.rnn_layers:\n",
    "            h = layer(h, training=training)\n",
    "        return self.dense(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b7e1d8-61af-4dc8-97be-a70f3d0d9dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, num_layers, output_dim, module_name):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Determine which RNN cell to use based on the module_name\n",
    "        if module_name == 'gru':\n",
    "            rnn_cell = layers.GRU\n",
    "        elif module_name == 'lstm':\n",
    "            rnn_cell = layers.LSTM\n",
    "        elif module_name == 'lstmLN':\n",
    "            rnn_cell = layers.lstmLN\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN module: {module_name}\")\n",
    "\n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(hidden_units, return_sequences=True)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = layers.Dense(output_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.rnn_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "189a570e-0d90-4c27-a594-a221ed6cf688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, num_layers, output_dim, module_name):\n",
    "        super(Supervisor, self).__init__()\n",
    "\n",
    "        # Determine which RNN cell to use based on the module_name\n",
    "        if module_name == 'gru':\n",
    "            rnn_cell = layers.GRU\n",
    "        elif module_name == 'lstm':\n",
    "            rnn_cell = layers.LSTM\n",
    "        elif module_name == 'lstmLN':\n",
    "            rnn_cell = layers.lstmLN\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN module: {module_name}\")\n",
    "\n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(hidden_units, return_sequences=True)\n",
    "            for _ in range(num_layers - 1)\n",
    "        ]\n",
    "        self.dense = layers.Dense(output_dim, activation='sigmoid')\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.rnn_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f817f7a-eb50-487c-8700-22687bfeb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, hidden_units, num_layers, module_name='gru'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_units: number of units per layer\n",
    "            num_layers: number of layers\n",
    "            module_name: 'gru' or 'lstm'\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Determine which RNN cell to use based on the module_name\n",
    "        if module_name == 'gru':\n",
    "            rnn_cell = layers.GRU\n",
    "        elif module_name == 'lstm':\n",
    "            rnn_cell = layers.LSTM\n",
    "        elif module_name == 'lstmLN':\n",
    "            rnn_cell = layers.lstmLN\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RNN module: {module_name}\")\n",
    "\n",
    "        self.rnn_layers = [\n",
    "            rnn_cell(hidden_units, return_sequences=True)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = layers.Dense(1, activation=None) # 1 score per time step\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for layer in self.rnn_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return self.dense(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56b0fde0-50a4-4aa6-a403-0beb4ab030c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Assuming 'mse' is a function that calculates mean_squared_error\n",
    "def embedding_loss(x, x_tilde, G_loss_S):\n",
    "    \"\"\"Calculates the final embedder loss as in the original code.\n",
    "    \n",
    "    Args:\n",
    "      x: The original data batch.\n",
    "      x_tilde: The reconstructed data batch.\n",
    "      G_loss_S: The supervised loss value.\n",
    "    \"\"\"\n",
    "    # 1. Calculate the base reconstruction loss (E_loss_T0)\n",
    "   # E_loss_T0 = mse(x, x_tilde)\n",
    "\n",
    "    # after (targets-only)\n",
    "    E_loss_T0 = mse(x_batch[..., y_idx], x_tilde[..., y_idx])\n",
    "    # 2. Apply the square root and the coefficient (E_loss0)\n",
    "    E_loss0 = 10 * tf.sqrt(E_loss_T0)\n",
    "    \n",
    "    # 3. Add the supervised loss component to get the final loss (E_loss)\n",
    "    E_loss = E_loss0 + 0.1 * G_loss_S\n",
    "    \n",
    "    return E_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db9fea32-75c9-49ed-a137-7236b6ab765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_loss(H, H_hat_supervised):\n",
    "    \"\"\"Supervised loss = MSE over ALL dims (B, T-1, H).\"\"\"\n",
    "    tgt = H[:, 1:, :]            # H at t+1\n",
    "    pred = H_hat_supervised[:, :-1, :]  # S(H) at t\n",
    "    return tf.reduce_mean(tf.square(tgt - pred))  # scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02b65a57-ff7d-4a62-a1de-48c490345816",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True) # Set to True for raw output\n",
    "\n",
    "def discriminator_loss(H, H_hat, E_hat, discriminator, gamma=1):\n",
    "    \"\"\"Adversarial loss for the discriminator, matching the provided reference.\n",
    "\n",
    "    Args:\n",
    "        H: The real latent sequences (from the embedder).\n",
    "        H_hat: The fake latent sequences (from the supervisor).\n",
    "        E_hat: The raw fake latent sequences (from the generator).\n",
    "        gamma: The weight for the D_loss_fake_e term.\n",
    "    \"\"\"\n",
    "    # 1. Get discriminator outputs\n",
    "    Y_real = discriminator(H)\n",
    "    Y_fake = discriminator(H_hat)\n",
    "    Y_fake_e = discriminator(E_hat)\n",
    "\n",
    "    # 2. Compute loss for each component using from_logits=True\n",
    "    D_loss_real = bce(tf.ones_like(Y_real), Y_real)\n",
    "    D_loss_fake = bce(tf.zeros_like(Y_fake), Y_fake)\n",
    "    D_loss_fake_e = bce(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
    "\n",
    "    # 3. Combine the losses with the gamma parameter\n",
    "    D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "\n",
    "    return D_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08908f75-d449-4088-82d9-1aa4eca25e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(\n",
    "    hidden_units=parameters['hidden_dim'],\n",
    "    num_layers= parameters['num_layers'],\n",
    "    output_dim=parameters['hidden_dim'],\n",
    "    module_name=parameters['module']\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "recovery = Recovery(\n",
    "    hidden_units=parameters['hidden_dim'],\n",
    "    num_layers= parameters['num_layers'],\n",
    "    output_dim=parameters['dim'],\n",
    "    module_name=parameters['module']\n",
    ")\n",
    "\n",
    "supervisor = Supervisor(\n",
    "    hidden_units=parameters['hidden_dim'],\n",
    "    num_layers=parameters['num_layers'],\n",
    "    output_dim=parameters['hidden_dim'],\n",
    "    module_name=parameters['module']\n",
    ")\n",
    "\n",
    "generator = Generator(\n",
    "    hidden_units=parameters['hidden_dim'],\n",
    "    num_layers=parameters['num_layers'],\n",
    "    output_dim=parameters['hidden_dim'],\n",
    "    module_name=parameters['module']\n",
    ")\n",
    "discriminator = Discriminator(\n",
    "    hidden_units=parameters['hidden_dim'],\n",
    "    num_layers=parameters['num_layers'],\n",
    "    module_name=parameters['module']\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "autoencoder_optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'], beta_1=0.9)\n",
    "supervisor_optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'], beta_1=0.9)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'], beta_1=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=parameters['learning_rate'], beta_1=0.9 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed1dfb4a-f6a6-4c89-85f1-db3c8d3f2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "def train_autoencoder_step(x_batch, y_idx):\n",
    "    y_idx_tf = tf.constant(y_idx, dtype=tf.int32)  # [k]\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x_batch, training=True)\n",
    "        x_tilde = recovery(h, training=True)\n",
    "\n",
    "        x_y   = tf.gather(x_batch, y_idx_tf, axis=-1)   # (B,T,k)\n",
    "        xt_y  = tf.gather(x_tilde, y_idx_tf, axis=-1)   # (B,T,k)\n",
    "        e_loss_t0 = mse(x_y, xt_y)                      # targets-only\n",
    "\n",
    "        loss = 10.0 * tf.sqrt(e_loss_t0 + 1e-8)\n",
    "\n",
    "    vars_ = embedder.trainable_variables + recovery.trainable_variables\n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    autoencoder_optimizer.apply_gradients(zip(grads, vars_))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00f097cc-329f-4274-b900-24e4e9e21aff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_timegan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m BATCH_SIZE = parameters[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m] \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Build a fresh, repeating dataset + iterator for the EMBEDDER/AUTOENCODER phase\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m embedder_dataset = make_dataset(\u001b[43mX_timegan\u001b[49m, BATCH_SIZE)\n\u001b[32m      6\u001b[39m embedder_batch_iter = \u001b[38;5;28miter\u001b[39m(embedder_dataset)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mStart Embedding Network Training\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_timegan' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the iteration count from your parameters\n",
    "embedder_iterations = parameters['embedder_iterations']\n",
    "BATCH_SIZE = parameters['batch_size'] \n",
    "# Build a fresh, repeating dataset + iterator for the EMBEDDER/AUTOENCODER phase\n",
    "embedder_dataset = make_dataset(X_timegan, BATCH_SIZE)\n",
    "embedder_batch_iter = iter(embedder_dataset)\n",
    "\n",
    "print('Start Embedding Network Training')\n",
    "\n",
    "e_loss_history = []\n",
    "step = 0\n",
    "\n",
    "# Train until we hit the target number of iterations\n",
    "while step < embedder_iterations:\n",
    "    x_batch = next(embedder_batch_iter)  # next reshuffled batch\n",
    "    # Your AE step returns 10 * sqrt(MSE)\n",
    "    step_e_loss0 = float(train_autoencoder_step(x_batch , y_idx))\n",
    "    e_loss_history.append(step_e_loss0)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        # Reference prints sqrt(MSE); you logged 10*sqrt(MSE) â†’ divide by 10\n",
    "        comparable_loss = np.mean(e_loss_history) / 10.0\n",
    "        print(f'step: {step}/{embedder_iterations}, e_loss: {comparable_loss:.4f}')\n",
    "        e_loss_history = []  # reset window\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print('Finish Embedding Network Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb4cfc-b3d4-4c8e-a8c0-b4b618947440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After AE pretrain to control sup LR\n",
    "embedder.trainable = False  # freeze E for supervisor warmup\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "lr_sched = PiecewiseConstantDecay(boundaries=[2000], values=[1e-3, 3e-4])\n",
    "\n",
    "# fresh optimizer for S (replaces the earlier constant-lr one)\n",
    "supervisor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_sched, beta_1=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18e7c4-e88a-4cd2-9bf8-2e6c4af9ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervisor_step(x_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        H = embedder(x_batch, training=False)\n",
    "        H = tf.stop_gradient(H)                 # keep E frozen\n",
    "\n",
    "        H_in  = H[:, :-1, :]                    # input timesteps\n",
    "        H_tgt = H[:,  1:, :]                    # target next-step\n",
    "\n",
    "        H_pred = supervisor(H_in, training=True)  # shape (B, T-1, hidden_dim)\n",
    "        loss = tf.reduce_mean(tf.square(H_tgt - H_pred))  # scalar MSE\n",
    "\n",
    "    grads = tape.gradient(loss, supervisor.trainable_variables)\n",
    "    supervisor_optimizer.apply_gradients(zip(grads, supervisor.trainable_variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "427788d8-71b6-4f93-b410-6b1e34f0c0e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_timegan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m supervisor_iterations =  parameters[\u001b[33m'\u001b[39m\u001b[33msupervisor_iterations\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# fresh dataset + iterator for Supervisor (donâ€™t reuse AE iterator)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sup_ds = make_dataset(\u001b[43mX_timegan\u001b[49m, BATCH_SIZE)\n\u001b[32m      7\u001b[39m sup_it = \u001b[38;5;28miter\u001b[39m(sup_ds)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStart Training with Supervised Loss Only\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_timegan' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Supervisor-only training loop (repeat + RMSE logging) ----\n",
    "supervisor_iterations =  parameters['supervisor_iterations']\n",
    "\n",
    "\n",
    "# fresh dataset + iterator for Supervisor (donâ€™t reuse AE iterator)\n",
    "sup_ds = make_dataset(X_timegan, BATCH_SIZE)\n",
    "sup_it = iter(sup_ds)\n",
    "\n",
    "print(\"Start Training with Supervised Loss Only\")\n",
    "s_hist = []\n",
    "for step in range(supervisor_iterations):  # e.g., 10_000\n",
    "    x_batch = next(sup_it)\n",
    "    step_mse = float(train_supervisor_step(x_batch))     # your S step (with stop_gradient)\n",
    "    s_hist.append(step_mse)\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"step: {step}/{supervisor_iterations}, s_loss: {(np.mean(s_hist))**0.5:.4f}\")\n",
    "        s_hist = []\n",
    "print(\"Finish Training with Supervised Loss Only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08295b-46c7-4db1-85c1-671edaac6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def moment_loss(x, x_hat, y_idx):\n",
    "    # targets-only slice\n",
    "    y_idx_tf = tf.constant(y_idx, tf.int32)\n",
    "    x_y      = tf.gather(x,      y_idx_tf, axis=-1)   # (B,T,k)\n",
    "    x_hat_y  = tf.gather(x_hat,  y_idx_tf, axis=-1)   # (B,T,k)\n",
    "\n",
    "    # moments over batch\n",
    "    mean_hat, var_hat = tf.nn.moments(x_hat_y, axes=[0])\n",
    "    mean_x,   var_x   = tf.nn.moments(x_y,     axes=[0])\n",
    "\n",
    "    std_hat = tf.sqrt(var_hat + 1e-6)\n",
    "    std_x   = tf.sqrt(var_x   + 1e-6)\n",
    "\n",
    "    G_loss_V1 = tf.reduce_mean(tf.abs(std_hat  - std_x))   # std diff\n",
    "    G_loss_V2 = tf.reduce_mean(tf.abs(mean_hat - mean_x))  # mean diff\n",
    "    return G_loss_V1 + G_loss_V2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed5424-dea3-4818-ae97-7392047da76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "print(\"ðŸ”„ Warm-up (Generator only; no adversarial)\")\n",
    "\n",
    "# One RNG for reproducible fresh noise each step\n",
    "tf_gen = tf.random.Generator.from_seed(42)\n",
    "\n",
    "# Fresh, repeating real-data iterator (donâ€™t reuse AE iterator)\n",
    "warmup_real_ds  = make_dataset(X_timegan.astype(np.float32), parameters['batch_size'])\n",
    "warmup_real_it  = iter(warmup_real_ds)\n",
    "\n",
    "# Freeze supervisor during G warm-up (optional but recommended)\n",
    "supervisor.trainable = False\n",
    "\n",
    "# Weights (typical TimeGAN-style scaling)\n",
    "lambda_s = 100.0   # supervised on generated latents\n",
    "lambda_v = 100.0   # two-moment loss in data space\n",
    "\n",
    "warmup_steps = 500  # 200â€“1000 is usually enough\n",
    "g_losses_Shat, g_losses_V, g_losses_total = [], [], []\n",
    "\n",
    "for step in range(warmup_steps):\n",
    "    real_batch = next(warmup_real_it)\n",
    "    real_batch = tf.cast(real_batch, tf.float32)\n",
    "\n",
    "    Z = sample_noise(parameters['batch_size'], parameters['seq_len'], parameters['z_dim'])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Latent path from noise\n",
    "        E_hat  = generator(Z, training=True)               # [B, T, hidden]\n",
    "        H_hat  = supervisor(E_hat, training=False)         # [B, T, hidden]  (S frozen)\n",
    "        X_hat  = recovery(H_hat, training=True)            # [B, T, dim]\n",
    "\n",
    "        # Supervised dynamics on generated latent (teacher forcing on itself)\n",
    "        H_hat_in  = H_hat[:, :-1, :]\n",
    "        H_hat_tgt = H_hat[:,  1:, :]\n",
    "        H_hat_pred = supervisor(H_hat_in, training=False)  # S frozen\n",
    "        G_loss_S_hat = tf.reduce_mean(tf.square(H_hat_tgt - H_hat_pred))\n",
    "\n",
    " \n",
    "\n",
    "        G_loss_V = moment_loss(real_batch, X_hat, y_idx)\n",
    "\n",
    "        # Total warm-up loss (no adversarial yet)\n",
    "        G_total = lambda_s * G_loss_S_hat + lambda_v * G_loss_V\n",
    "\n",
    "\n",
    "    # Update ONLY the generator\n",
    "    g_vars = generator.trainable_variables\n",
    "    g_grads = tape.gradient(G_total, g_vars)\n",
    "    generator_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    # Logging buffers\n",
    "    g_losses_Shat.append(float(G_loss_S_hat))\n",
    "    g_losses_V.append(float(G_loss_V))\n",
    "    g_losses_total.append(float(G_total))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Warmup {step}/{warmup_steps} | \"\n",
    "              f\"G_S_hat: {np.mean(g_losses_Shat[-100:]):.6f} | \"\n",
    "              f\"G_V: {np.mean(g_losses_V[-100:]):.6f} | \"\n",
    "              f\"G_total: {np.mean(g_losses_total[-100:]):.6f}\")\n",
    "\n",
    "# (optional) unfreeze supervisor for joint training\n",
    "supervisor.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c368bec-9f60-4b69-96e0-891e3226915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################################\n",
    "print(\"ðŸ”„ Warm-up (Generator only; no adversarial)\")\n",
    "\n",
    "# One RNG for reproducible fresh noise each step\n",
    "tf_gen = tf.random.Generator.from_seed(42)\n",
    "\n",
    "# Fresh, repeating real-data iterator (donâ€™t reuse AE iterator)\n",
    "warmup_real_ds  = make_dataset(X_timegan.astype(np.float32), parameters['batch_size'])\n",
    "warmup_real_it  = iter(warmup_real_ds)\n",
    "\n",
    "# Freeze supervisor during G warm-up (optional but recommended)\n",
    "supervisor.trainable = False\n",
    "\n",
    "# Weights (typical TimeGAN-style scaling)\n",
    "lambda_s = 100.0   # supervised on generated latents\n",
    "lambda_v = 100.0   # two-moment loss in data space\n",
    "\n",
    "warmup_steps = 500  # 200â€“1000 is usually enough\n",
    "g_losses_Shat, g_losses_V, g_losses_total = [], [], []\n",
    "\n",
    "for step in range(warmup_steps):\n",
    "    real_batch = next(warmup_real_it)\n",
    "    real_batch = tf.cast(real_batch, tf.float32)\n",
    "\n",
    "    Z = sample_noise(parameters['batch_size'], parameters['seq_len'], parameters['z_dim'])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Latent path from noise\n",
    "        E_hat  = generator(Z, training=True)               # [B, T, hidden]\n",
    "        H_hat  = supervisor(E_hat, training=False)         # [B, T, hidden]  (S frozen)\n",
    "        X_hat  = recovery(H_hat, training=True)            # [B, T, dim]\n",
    "\n",
    "        # Supervised dynamics on generated latent (teacher forcing on itself)\n",
    "        H_hat_in  = H_hat[:, :-1, :]\n",
    "        H_hat_tgt = H_hat[:,  1:, :]\n",
    "        H_hat_pred = supervisor(H_hat_in, training=False)  # S frozen\n",
    "        G_loss_S_hat = tf.reduce_mean(tf.square(H_hat_tgt - H_hat_pred))\n",
    "\n",
    " \n",
    "\n",
    "        G_loss_V = moment_loss(real_batch, X_hat, y_idx)\n",
    "\n",
    "        # Total warm-up loss (no adversarial yet)\n",
    "        G_total = lambda_s * G_loss_S_hat + lambda_v * G_loss_V\n",
    "\n",
    "\n",
    "    # Update ONLY the generator\n",
    "    g_vars = generator.trainable_variables\n",
    "    g_grads = tape.gradient(G_total, g_vars)\n",
    "    generator_optimizer.apply_gradients(zip(g_grads, g_vars))\n",
    "\n",
    "    # Logging buffers\n",
    "    g_losses_Shat.append(float(G_loss_S_hat))\n",
    "    g_losses_V.append(float(G_loss_V))\n",
    "    g_losses_total.append(float(G_total))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Warmup {step}/{warmup_steps} | \"\n",
    "              f\"G_S_hat: {np.mean(g_losses_Shat[-100:]):.6f} | \"\n",
    "              f\"G_V: {np.mean(g_losses_V[-100:]):.6f} | \"\n",
    "              f\"G_total: {np.mean(g_losses_total[-100:]):.6f}\")\n",
    "\n",
    "# (optional) unfreeze supervisor for joint training\n",
    "supervisor.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8683f0-b371-4969-83d9-34701e8482fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set once at top-level (because your D returns logits)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_generator_step(real_batch):\n",
    "    \"\"\"\n",
    "    Joint-phase generator step that matches the original TimeGAN behavior.\n",
    "    - Updates G + S together (as in G_solver optimizing g_vars + s_vars).\n",
    "    - Uses supervised loss on REAL H (S(H) vs H), like the original.\n",
    "    - Includes the two adversarial terms (on H_hat and on E_hat) and the moment loss.\n",
    "    \"\"\"\n",
    "    real_batch = tf.cast(real_batch, tf.float32)\n",
    "\n",
    "    # Fresh noise every step (you already defined sample_noise)\n",
    "    Z_mb = sample_noise(parameters['batch_size'],\n",
    "                        parameters['seq_len'],\n",
    "                        parameters['z_dim'])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # ---- Fake path (depends on G, so grads flow to G & S) ----\n",
    "        E_hat = generator(Z_mb, training=True)            # [B, T, hidden]\n",
    "        H_hat = supervisor(E_hat, training=True)          # [B, T, hidden]\n",
    "        #X_hat = recovery(H_hat, training=True)            # [B, T, dim]   ####if i want to return to before\n",
    "        X_hat = recovery(H_hat, training=False)\n",
    "\n",
    "        \n",
    "        # Discriminator predictions on fakes\n",
    "        Y_fake   = discriminator(H_hat, training=True)    # S(G(Z))\n",
    "        Y_fake_e = discriminator(E_hat, training=True)    # G(Z) pre-supervisor\n",
    "\n",
    "        # 1) Adversarial terms\n",
    "        g_loss_u   = bce(tf.ones_like(Y_fake),   Y_fake)\n",
    "        g_loss_u_e = bce(tf.ones_like(Y_fake_e), Y_fake_e)\n",
    "\n",
    "        # 2) Supervised term on REAL latent (keeps S aligned to real H)\n",
    "        H = embedder(real_batch, training=False)          # [B, T, hidden]\n",
    "        H_hat_supervise = supervisor(H, training=True)    # S(H)\n",
    "        g_loss_s = tf.reduce_mean(tf.square(H[:, 1:, :] - H_hat_supervise[:, :-1, :]))\n",
    "\n",
    "        # 3) Two-moment loss in data space\n",
    "        g_loss_v = moment_loss(real_batch, X_hat ,y_idx)\n",
    "\n",
    "        # ---- Total (original-style weights) ----\n",
    "        gamma = parameters.get('gamma', 1.0)  # weight for the G(Z) adversarial path\n",
    "        G_total = g_loss_u + gamma * g_loss_u_e \\\n",
    "                  + 100.0 * tf.sqrt(g_loss_s + 1e-8) \\\n",
    "                  + 100.0 * g_loss_v\n",
    "\n",
    "    # Update G + S together (like original G_solver over g_vars + s_vars)\n",
    "    vars_to_update = generator.trainable_variables + supervisor.trainable_variables\n",
    "    grads = tape.gradient(G_total, vars_to_update)\n",
    "    generator_optimizer.apply_gradients(zip(grads, vars_to_update))\n",
    "\n",
    "    # Return components for logging\n",
    "    return (G_total, g_loss_u, g_loss_u_e, g_loss_s, g_loss_v)\n",
    "\n",
    "\n",
    "\n",
    "# after (recommended for stability during G step)\n",
    "X_hat = recovery(H_hat, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc94ce-1241-48dd-8ee9-d1f91222c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding more condition to prevent d from loosing power\n",
    "FREEZE_STEPS = 1000              # e.g., 500â€“1000\n",
    "embedder.trainable = False       # freeze E for warmup\n",
    "\n",
    "print('Start Joint Training')\n",
    "\n",
    "iterations   = parameters.get('joint_iterations', 3000)\n",
    "log_interval = parameters.get('log_interval', 100)\n",
    "disc_thresh  = 1.8\n",
    "\n",
    "# Iterator over real data\n",
    "joint_dataset = make_dataset(X_timegan.astype(np.float32), parameters['batch_size'])\n",
    "joint_iter    = iter(joint_dataset)\n",
    "\n",
    "# For logging\n",
    "step_g_loss_u = step_g_loss_u_e = step_g_loss_s = step_g_loss_v = 0.0\n",
    "step_e_loss_t0 = 0.0\n",
    "step_d_loss = 0.0\n",
    "\n",
    "for itt in range(iterations):\n",
    "    # --- TTUR nudge for D (first ~500 iters) ---\n",
    "   # top of loop\n",
    "    if itt < 500:\n",
    "      lr_D = 4e-4   # brief boost (already in place)\n",
    "    elif itt < 1500:\n",
    "      lr_D = 2e-4   # COOL DOWN\n",
    "    else:\n",
    "      lr_D = 1e-4   # steady-low\n",
    "    try:\n",
    "      discriminator_optimizer.learning_rate.assign(lr_D)\n",
    "    except Exception:\n",
    "      import tensorflow.keras.backend as K\n",
    "      K.set_value(discriminator_optimizer.learning_rate, lr_D)\n",
    "\n",
    "    # --- Unfreeze embedder exactly at FREEZE_STEPS ---\n",
    "    if (itt == FREEZE_STEPS) and (embedder.trainable is False):\n",
    "        embedder.trainable = True\n",
    "\n",
    "    # ---- (Generator + (optional) Embedder) x 2 ----\n",
    "    for _ in range(2):\n",
    "        # Generator step (updates G+S inside)\n",
    "        x_batch = next(joint_iter)\n",
    "        G_total, step_g_loss_u, step_g_loss_u_e, step_g_loss_s, step_g_loss_v = train_generator_step(x_batch)\n",
    "\n",
    "        # Embedder step only after unfreeze\n",
    "        if embedder.trainable:\n",
    "            x_batch_e = next(joint_iter)\n",
    "            step_e_loss_t0 = train_embedder_step(x_batch_e, y_idx)  # returns recon MSE\n",
    "\n",
    "    # ---- Discriminator: probe + gated updates ----\n",
    "    x_batch_d = next(joint_iter)\n",
    "    d_check = float(probe_discriminator_loss_min(x_batch_d))\n",
    "\n",
    "    # If D is clearly off, do an extra D step ONCE in a while (your modulo-20 rule)\n",
    "    if ((d_check > 2.0) or (float(step_g_loss_u) > 3.0)) and (itt % 50 == 0):\n",
    "        step_d_loss, *_ = train_discriminator_step(x_batch_d)\n",
    "        x_batch_d2 = next(joint_iter)                  # fresh batch\n",
    "        step_d_loss, *_ = train_discriminator_step(x_batch_d2)\n",
    "    else:\n",
    "        # Normal gate\n",
    "        if d_check > disc_thresh:\n",
    "            step_d_loss, *_ = train_discriminator_step(x_batch_d)\n",
    "        else:\n",
    "            step_d_loss = d_check  # just log the probe\n",
    "\n",
    "    # ---- Logging (reference style) ----\n",
    "    if itt % log_interval == 0:\n",
    "        print('step: ' + str(itt) + '/' + str(iterations) +\n",
    "              ', d_loss: '   + f'{float(step_d_loss):.4f}' +\n",
    "              ', g_loss_u: ' + f'{float(step_g_loss_u):.4f}' +\n",
    "              ', g_loss_s: ' + f'{np.sqrt(float(step_g_loss_s)+1e-8):.4f}' +\n",
    "              ', g_loss_v: ' + f'{float(step_g_loss_v):.4f}' +\n",
    "              ', e_loss_t0: '+ f'{np.sqrt(float(step_e_loss_t0)+1e-8):.4f}')\n",
    "\n",
    "print('Finish Joint Training')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
